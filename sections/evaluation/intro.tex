To evaluate the performance and capability of the devised algorithm, as well as compare different implementations and sets of predicates against each other, the agent can be run on a set of levels.
A script then logs relevant information, such that runs with different agent versions can be compared.
The most important metric to be logged is which levels the agent is capable of beating at all and the highest achieved score per level.
Other metrics include which strategies where used for each shot.
This is especially important to determine the frequency with which \acs{CBR} cases are found to be applicable and the success rate of adaptation.

All evaluations were performed on a system with 16GB of RAM and a 2.6 Ghz hexa-core CPU.

Some of the components are tested in isolation, while some tests were run with a a combination of modules. Details are explained below.