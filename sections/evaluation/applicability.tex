Finding the case from the database that best matches a given scene is difficult, mainly due to the high number of possible combinations of objects that could match.
To further investigate this, the agent was run for 60 minutes in a create-only mode, where it only performed random shots and generated cases for shots that killed one or more pigs but did not target them directly.
This generated 60 cases and 1 Megabyte worth of Prolog rules. Each case is associated with affected objects, on average 20 per case $k$ (min: 2, max: 55, median: 20.5), see figure \ref{fig:objects-per-case}.

With an average of 40.67 objects per level $n$ over the test set, this means that
$8.1*10^{52}$ possible combinations ($k^n$) exist that would have to be checked for an average case and an average level if no hard constraints are considered apart from the requirement that every object has to have a match.
Additionally requiring all objects to be unique, i.e. one object in the level cannot be matched to multiple objects in the same case, reduces this number to
$5.3*10^{29}$ (binomial coefficient), which, although significantly less, is still far too big for all configurations to be checkable.

\asfigure{fig:objects-per-case}{data/objects_per_case}{Distribution of objects per case}{10}

As discussed previously, a possible mitigation is the introduction of additional hard constraints, such as requiring objects to have the same material and shape that they had in the original scenario.
This has the additional property of discarding entire cases, e.g. when a case requires ice objects but the level does not include any.
This is helpful from a runtime optimization perspective but limits the generalizability of cases, as for some cases the actual material of some or all objects might not matter.
How much this cuts down the number of possibilities is hard to calculate, since it depends on the distribution of object types in the levels and therefore cannot be averaged over the entire set as easily.
Another way to reduce the number of allowed assignments is to require the target object to be hittable.
Again, the numerical impact of this is highly situational, but it is worth implementing this either way:
There is little use in finding an assignment with a high score if the target object cannot be hit due to obstacles.

If one would calculate the number of possible assignments for a case over every level in the test set, take the mean of those values and then the median of the means for every case, this would work out to $3.6*10^{18}$. Again, a significant but insufficient improvement.
A different, possibly more useful way to look at this would be the sum of configurations over all cases for a given level. This gives an upper bound of configurations one had to check if every case possible configuration for every case should be evaluated for a level. On average, this would work out to $4.8*10^{54}$ (min: 480 max: $9.8*10^{55}$ median: $4.5*10^{17}$).

In addition to the greatly increased number of possible permutations, each object also increases the cost of checking a configuration by adding relations.
Since for every object the position relative to every other object is stored, this causes a quadratic growth. Although the ERA has more predicates, this does not reflect here since the relation between two objects still consists of exactly one predicate on each axis.


In the implementation section, the first proposed mitigation to handle this complexity was a stochastic approach: Generate $n$ random assignments for each case and evaluate them. but how large would $n$ need to be for this to be useful?
Let's look at an example: A case with manageable complexity, so that every combination can be generated and evaluated against the situation from which the case was generated shows: even when a perfect match is available, the probability of randomly finding a configuration that satisfies even half of all predicates is 33.71\%, dropping to 0.47\% if more than 75\% of predicates must be satisfied.

\asfigure{fig:one-case}{data/onecase_distribution}{Distribution of score for a case on a scene with a perfect match available}{7}

The solution for this is a more directed approach, \ac{MCH} and \ac{WMCH}. To judge the improvement, again a database of cases was generated by the agent.
For each of the 21 levels, the best matching case was calculated with the random approach and both heuristic approaches. For every level, the time it took the algorithm to arrive at a solution and the score of the solution was collected, i.e. the percentage of matching predicates for the assignment. This was repeated for different amounts of iterations. One iteration of the random approach is not directly comparable to one iteration of \ac{MCH} or \ac{WMCH}, as those check multiple matches for one variable in each iteration.

Indeed, the results show that the heuristic algorithms outperform the random approach on average score, but take longer for the same amount of iterations. Since their average score result does not keep increasing past 50 iterations, see figure \ref{fig:score-by-algo}, and the time to check all cases is manageable, see figure \ref{fig:time-by-algo}, \ac{WMCH} with 50 iterations is used for the other evaluations unless otherwise specified.
While not noticeably outperforming \ac{MCH}, \ac{WMCH} achieves a good tradeoff between time and score, as can be seen in figure \ref{fig:algo-comparison}.

\asfigure{fig:score-by-algo}{data/score_by_algorithm}{Scores on all levels with n iterations. \ac{MCH} in red, \ac{WMCH} in blue and randomized in green}{15}
\asfigure{fig:time-by-algo}{data/time_by_algorithm}{Time used to check all cases for n iterations}{15}
\asfigure{fig:algo-comparison}{data/algo_comparison}{Score vs. Time taken on all levels}{15}
