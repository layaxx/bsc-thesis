The sets of relations are evaluated primarily by the score the agent achieves while using them. Additionally, the success rate of applied cases is compared.
In this context, the success of a case is considered total, if every pig the case promises to destroy is destroyed by the shot. 
If a case promises to destroy multiple targets, but the shot only destroys some, the application of the case is considered partially successful. 
If none of the pigs are destroyed, the application was unsuccessful.
Note that every case must promise to destroy at least one pig, as it would not have been added to the database otherwise.

To keep separate runs as comparable as possible, a database of cases is first generated by running the agent in create-only mode.
For every case, enough information is saved such that predicates can be generated after the agent has finished. This means that the target object as well as the pre and post-shot screenshots are saved for every generated case.

For each set of predicates, the corresponding Prolog rules can now be generated by loading the images, generating the cases as the agent would during a normal run and writing the prolog rules to the database. The agent can then run in CBR-only mode.
This means that the agent tries to find a matching CBR case from the database and executes the best available. If no case matches better than a threshold, the agent targets a random pig directly. In this mode, no new cases are generated and cases are not updated, to limit randomness.
To further improve comparability, the Level selection module is configured to first iteratively play every level before any level can be attempted again.
Due to stochastic elements in the scene matching, which have been discussed earlier, randomness cannot be entirely avoided, however.

Each run had a time limit of 60 minutes.
Applicability of cases was determined with 50 iterations of the \ac{WMCH} algorithm, with a time limit of 30 seconds per situation. Every action performed by the agent is logged allowing the run to be analyzed afterward.
Relevant data points include the strategies that were considered and which strategy was chosen for each shot, the results of a shot, expected and actual effects as well as mapping of old to new IDs for CBR shots and the score if a level is cleared.

For this purpose, the quality is judged by the maximum score the agent achieves and by whether a case kills the pigs it promised to, regardless of other objects that should have been affected.

Figure \ref{fig:eval-strat-frequency} shows that all versions use cases from \ac{CBR} for about two-thirds of shots. As expected, the least constraining set of relations, \ac{RIA}, leads to more matching cases. This strategy also managed to beat the highest number of levels, although the difference between versions is small, see figure \ref{fig:eval-number-of-cleared}. Notably, this was the only agent to beat Level 12, see figure \ref{fig:eval-cleared}.

With the exception of \ac{IA}, the derivatives of Interval Algebras where the before predicate is split based on whether objects are close enough to hit each other, performed worse than the base versions by score, number of levels cleared and percentage of totally or partially successful applications. This is an unexpected result and, in combination with the good performance of \ac{RIA}, suggests that the \ac{CBR} strategy benefits more from a higher level of abstraction.

Different sets of relations mostly lead to the same case being the best fit for a given scenario. This analysis can only be done for the first shot of any given level because only then the situation is known to be equal between runs. This can be seen in figure \ref{fig:eval-best-case}. The log from the case generation shows that for example in Level 4, two different perfect matches are available. both of which were found by different agents.


\asfigure{fig:eval-all}{data/eval_all}{Maximum Score by strategy on each level}{10}
\asfigure{fig:eval-cleared}{data/eval_cleared}{cleared levels for each strategy}{10}
\asfigure{fig:eval-number-of-cleared}{data/eval_number_of_cleared}{number of cleared levels by strategy}{10}
\asfigure{fig:eval-scores-no-zero}{data/eval_scores_no_zero}{Maximum Score on cleared levels}{15}
\asfigure{fig:eval-sum-of-scores}{data/eval_sum_of_scores}{Sum of scores over every level}{15}
\asfigure{fig:eval-strat-frequency}{data/eval_strat_frequency}{Frequency of shots that were considered and executed}{15}
\asfigure{fig:eval-success}{data/eval_success}{Frequency of \ac{CBR} shots destroying some or all pigs they were expected to}{15}
\asfigure{fig:eval-best-case}{data/eval_best_case}{Cases found to be best fit for the first shot in each level}{15}